---
types: ["macro"]
date: 2025-04-04T12:06:53-04:00
title: "two things that bug me about arguments that generative AI is inevitable or whatever"
tags: ["generative AI","Jacques Ellul","morals","ethics","The Technological Society","NCII","David Graeber","David Wengrow","The Dawn of Everything"]
---
I don't know that "inevitable" is the right word to use in the title of this post. What I'm trying to evoke is that specific argument about generative AI that now that it's here, there's no going back, so the only real/responsible/whatever choice is to learn to use it properly, teach others to use it, accept it as part of life, etc. These are the arguments that the world is forever changed and that there's no going back—that the genie is out of the bottle so we might as well harness it.

To be honest, I do find that argument somewhat compelling. Compelling enough that despite my personal antipathy toward all things generative AI, I celebrated when our department's new hire was an AI researcher and took steps in my role as program coordinator to schedule another instructor's special topics course on AI for next fall. That said, I see two real problems with that argument that really bug me, and I want to lay them out here.

## moral surrender

First, this argument gets used to sidestep moral concerns about generative AI. At best, it acknowledges those moral concerns before saying that they are outweighed by the simple presence of generative AI ("yes, there are problems, but these tools aren't going away, so we ought to use them"); at worst, it outright dismisses those arguments as irrelevant ("technological progress marches on, so please stop your hand-wringing"). 

I've been reading a lot of Jacques Ellul recently, and it's remarkable how prescient his writing is on this subject. On multiple occasions, he argues that our moral frameworks are being replaced by a unique concern for efficient technique and technological progress:

Here's an excerpt from *The Technological Society*:

> Modern society is, in fact, conducted on the basis of purely technical considerations. But when men found themselves going counter to the human factor, they reintroduced—and in an absurd way all manner of moral theories related to the rights of man, the League of Nations, liberty, justice. None of that has any more importance than the ruffled sunshade of McCormick's first reaper. When these moral flourishes overly encumber technical progress, they are discarded—more or less speedily, with more or less ceremony, but with determination nonetheless. This is the state we are in today.

Here's another passage, from a 1980 essay *The Power of Technique and the Ethics of Non-Power*:

> Technique itself has become a virtue and (paralleling the scientific community's attempt to found a morality on scientific integrity) proposed the values of normalcy, efficiency, industriousness, professional ethics, and devotion to collective projects as values. In each case, everything is subordinated to efficiency, in other words, geared toward adaptation. Hence technological morality consists in allowing technique free play, and if traditional values are invoked, it is usually for another reason than to justify the primacy of technique[.]

Don't get me wrong: I'm sure some moral concerns about generative AI are unfounded and that others are, indeed, outweighed by some of the benefits that these tools provide. However, a straight up moral surrender—accepting the tool because it's here without even discussing the issues—strikes me as deeply problematic.

## abandoning hope

Second, this argument bothers me for the way that it assumes that humanity is incapable of change; that we are incapable of replacing the world we live in with a better one. Perhaps that last part—"a better one"—makes a lot of difference here; going back to my last point, perhaps those arguing for the inevitability of generative AI don't actually see it as making the world substantively worse. If so—and again, going back to my last point—that's the argument that needs to be made (and, to be honest, they'yre going to have to work hard to sell me on the idea that easy access to [nudifier apps](https://www.404media.co/what-was-she-supposed-to-report-police-report-shows-how-a-high-school-deepfake-nightmare-unfolded/) in middle and high schools can be morally balanced out by whatever else). If the real argument is that yeah, the world sucks, but there's no reason to hope in changing it, that just feels nihilistic to me. 

Granted, things like a second Trump administration, the ongoing climate crisis, and plenty of other unfolding problems emphasize that we don't have a great track record with bringing about positive change in the world. Does that mean, though, that we should give up on hopes that we can improve in those areas? And surely if we have enough hope that we can put those genies back into their respective bottles, couldn't we do the same thing with generative AI? (Again, a lot here depends on whether someone sees the emergence of this tool as a negative development; again, this is why moral surrender and immediate pivot to "let's use it effectively" is such a problem).

Here, I want to draw on David Graeber and David Wengrow's *The Dawn of Everything*. It's a dense book, and I admit that I have trouble following all the ins and outs, but what I appreciate about their core thesis is that they reject any kind of deterministic view of human history. (In fact, this creates some interesting tensions with Ellul, who shares the anarchist bent of these authors—or at least Graeber). In their conclusion, they write:

> Why are we entertaining such ideas? Why does it seem so odd, even counter-intuitive, to imagine people of the remote past as making their own history (even if not under conditions of their own choosing)? Part of the answer no doubt lies in how we have come to defe science itself, and social science in particular.
> 
> Social science has been largely a study of the ways in which human beings are not free: the way that our actions and understands might be said to be determined by forces outside our control. An account which appears to show human beings collectively shaping their own destiny, or even expressing freedom for its own sake, will likely be written off as illusory, awaiting 'real' scientific explanation: or if none is forthcoming (why do people dance?), as outside the scope of social theory entirely.

This is a bold argument, and I love it. I'll be honest, I'm not terribly optimistic about humanity "collectively shaping their own destiny"—whether it's about averting climate catastrophe, putting generative AI back into the bottle, or anything else. I refuse to give up on hope in that possibility, though. 