---
types: ["macro"]
date: 2024-12-06T19:04:15-05:00
title: "trapped between generative AI and student surveillance"
tags: ["grading","assessment","generative AI","surveillance"]
---
We're getting to the end of the semester here at the University of Kentucky, which is my traditional time to get overly introspective about grading. There's a lot on my mind at the end of this semester, but one thing that has popped into my head tonight and that I think will be quick to write about is a dilemma that I'm facing this semester, when I've had faced more suspicions about student use of generative AI than in any previous semester. By way of context, my class policy is to: 1) discourage student use of generative AI, but 2) begrudgingly allow students to use it, but 3) require that they disclose its use. 

As my policy suggests, I don't want my students to use generative AI. I have deep ethical concerns about the technology, and I don't think that using generative AI is conducive to learning as I understand the phenomenon (one day, I need to write a different post on sociocultural learning, epistemology, and generative AI, but today is not that day). I technically allow for its use, but I'd really prefer that students not do so. 

I'm also deeply opposed to a culture of student surveillance. I won't use Proctorio, I don't use plagiarism checkers, and I try to adopt a posture of trusting students. This means that even though I suspect that some students are using generative AI without disclosing it, I'm hesitant to do anything about it unless I have really strong indications that this is happening. I have called out one student for not following the policy because there was an obviously hallucinated citation in their work, but when I suspected the same student of further violations, my case wasn't as airtight. There were suspicious errors in the citations that seemed to fit the pattern I'd seen a week or two earlier, but there was just enough plausibility that I didn't feel good about pressing the case.

As upset as I am about my students' potentially "getting away with" using generative AI without disclosing it, I wouldn't be happy with myself if I reneged on my commitment to avoiding student surveillance. Automated tools for "detecting" AI use suck, and I don't want to act on false positives or create a culture of permanent suspicion of students. Even case-by-case scrutiny can get dodgy. I don't have hard numbers on all of the plagiarism cases that I've pursued during my time at UK, but I do know that a disproportionate number of the cases that I've pursued involved students with non-European last names. I wasn't wrong in any of those cases, but I still worry about what that says about me as a teacher. So, when a student I know to be an immigrant turns in work that seems like it *could* have been written with generative AI, I have to stop and ask myself if I'm making harmful assumptions about students' levels of English fluency. I'd rather err on the side of trusting students than have my teaching be defined by persistent surveillanceâ€”especially when I may be more likely (despite my best efforts to the contrary) to wield that surveillance against my more marginalized students.

So, at a meta level, what I like even less about generative AI than its non-disclosed use by students is the fact that the most ethical response to generative AI that I've been able to come up with is one that requires me to be less hawkish in my disapproval of the technology than my base concerns about it would otherwise lead me to be. It's a stark reminder of the imbalance between instructors and tech companies in terms of both power to change things and ethical constraints. My ethical commitments as an educator make me skeptical about generative AI, but they also limit my ability to act on that skepticism with my students.
